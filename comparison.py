"""
Bu uygulama, farklÄ± veri iÅŸleme ve ÅŸifreleme yaklaÅŸÄ±mlarÄ±nÄ±n performanslarÄ±nÄ± 
karÅŸÄ±laÅŸtÄ±rmak amacÄ±yla oluÅŸturulmuÅŸtur. Veri sÄ±kÄ±ÅŸtÄ±rma (Huffman), boyut indirgeme (Autoencoder) 
ve ÅŸifreleme (AES, RSA) yÃ¶ntemlerinin farklÄ± kombinasyonlarÄ±nÄ±n zaman, bellek kullanÄ±mÄ± ve 
veri boyutu aÃ§Ä±sÄ±ndan performanslarÄ±nÄ± karÅŸÄ±laÅŸtÄ±rmalÄ± olarak analiz etmektir.


https://yzup-proje-comparison-o98cjdhnalrztsvmczvd9t.streamlit.app/
"""


import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import make_regression
from sklearn.preprocessing import MinMaxScaler
from Crypto.Cipher import AES, PKCS1_OAEP
from Crypto.PublicKey import RSA
from Crypto.Random import get_random_bytes
import heapq, psutil, os, time
from collections import defaultdict
import tracemalloc


# =========================
# HUFFMAN KODLAMA
# =========================
# KayÄ±psÄ±z veri sÄ±kÄ±ÅŸtÄ±rma yÃ¶ntemidir. SÄ±k tekrarlanan karakterlere daha kÄ±sa, 
# nadir olanlara daha uzun bit dizileri atar. Bu sayede veri boyutu azaltÄ±lÄ±r.
class HuffmanNode:
    def __init__(self, char, freq):
        self.char = char  # Karakter (veya byte deÄŸeri)
        self.freq = freq  # Frekans (kaÃ§ kez geÃ§tiÄŸi)
        self.left = None 
        self.right = None
    def __lt__(self, other):
        return self.freq < other.freq

def build_huffman_tree(freq_map):  # Frekans haritasÄ±ndan Huffman aÄŸacÄ± oluÅŸturma
    heap = [HuffmanNode(char, freq) for char, freq in freq_map.items()]
    heapq.heapify(heap)
    while len(heap) > 1:
        left = heapq.heappop(heap)
        right = heapq.heappop(heap)
        merged = HuffmanNode(None, left.freq + right.freq)
        merged.left = left
        merged.right = right
        heapq.heappush(heap, merged)
    return heap[0]

# Huffman aÄŸacÄ±ndan kod tablosunu Ã§Ä±karma (karakter -> bit dizisi)
def build_codes(node, prefix="", code_map=None):
    if code_map is None:
        code_map = {}
    if node:
        if node.char is not None:
            code_map[node.char] = prefix
        build_codes(node.left, prefix + "0", code_map)
        build_codes(node.right, prefix + "1", code_map)
    return code_map

# Huffman ile sÄ±kÄ±ÅŸtÄ±rma (veri -> bit dizisine -> baytlara)
def huffman_encode(data_bytes):
    freq_map = defaultdict(int)
    for b in data_bytes:
        freq_map[b] += 1
    root = build_huffman_tree(freq_map)
    code_map = build_codes(root)
    encoded_bits = ''.join(code_map[b] for b in data_bytes)
    # 8 bitlik bloklara bÃ¶lebilmek iÃ§in sÄ±fÄ±rla doldurma (padding)
    padded_encoded_bits = encoded_bits + '0' * ((8 - len(encoded_bits) % 8) % 8)
    byte_array = bytearray()
    for i in range(0, len(padded_encoded_bits), 8):
        byte_array.append(int(padded_encoded_bits[i:i+8], 2))
    return bytes(byte_array)

# =========================
# AES / RSA
# =========================
# AES iÃ§in veri bloklarÄ±nÄ± 16 byte'a pad etme
def pad(data_bytes):
    pad_len = 16 - (len(data_bytes) % 16)
    return data_bytes + bytes([pad_len]) * pad_len
# AES ÅŸifreleme (ECB modu). Blok ÅŸifreleme algoritmasÄ±dÄ±r. Simetrik anahtarlÄ± Ã§alÄ±ÅŸÄ±r, 
# yani ÅŸifreleme ve Ã§Ã¶zme iÅŸlemleri aynÄ± anahtarla yapÄ±lÄ±r. HÄ±zlÄ±dÄ±r ve Ã¶zellikle bÃ¼yÃ¼k veri bloklarÄ±nda etkilidir.
def encrypt_aes(data_bytes, key_bytes):
    cipher = AES.new(key_bytes, AES.MODE_ECB)
    return cipher.encrypt(pad(data_bytes))
# RSA ile veri ÅŸifreleme (uzunluk sÄ±nÄ±rlamasÄ± var!). Asimetrik bir ÅŸifreleme algoritmasÄ±dÄ±r. 
# AÃ§Ä±k ve Ã¶zel anahtar Ã§iftiyle Ã§alÄ±ÅŸÄ±r. Daha gÃ¼venlidir fakat bÃ¼yÃ¼k verilerde yavaÅŸtÄ±r ve daha fazla kaynak kullanÄ±r.
def encrypt_rsa(data_bytes, rsa_key):
    cipher = PKCS1_OAEP.new(rsa_key)
    # RSA OAEP ile ÅŸifrelenebilecek maksimum veri miktarÄ± sÄ±nÄ±rlÄ±
    return cipher.encrypt(data_bytes[:rsa_key.size_in_bytes() - 42])

# =========================
# AUTOENCODER 
# =========================
# 540 boyutlu girdiyi 16 boyuta kadar sÄ±kÄ±ÅŸtÄ±rÄ±p geri Ã§Ä±karan bir autoencoder. 
# Bir yapay sinir aÄŸÄ± modelidir. Girdi verisini daha kÃ¼Ã§Ã¼k boyutlu bir temsile (encoding) dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r 
# ve tekrar orijinal veriyi Ã¼retmeyi Ã¶ÄŸrenir. Bu projede boyut indirgeme ve veri sÄ±kÄ±ÅŸtÄ±rma iÃ§in kullanÄ±lmÄ±ÅŸtÄ±r.
class Autoencoder(nn.Module):
    def __init__(self):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(540, 256), nn.ReLU(),
            nn.Linear(256, 64), nn.ReLU(),
            nn.Linear(64, 16), nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(16, 64), nn.ReLU(),
            nn.Linear(64, 256), nn.ReLU(),
            nn.Linear(256, 540), nn.Sigmoid()
        )

    def forward(self, x):
        z = self.encoder(x)
        out = self.decoder(z)
        return out

# =========================
# COMPARISON TESTS
# =========================
def run_test(use_autoencoder=True):
    # 540 Ã¶zellikli rastgele veri Ã¼ret
    X, _ = make_regression(n_samples=1000, n_features=540, noise=0.1, random_state=42)
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    X_tensor = torch.FloatTensor(X_scaled)

     # Autoencoder ile veri sÄ±kÄ±ÅŸtÄ±rmasÄ± yapÄ±lacaksa eÄŸitim yap
    if use_autoencoder:
        autoencoder = Autoencoder()
        criterion = nn.MSELoss()
        optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)
        dataloader = DataLoader(TensorDataset(X_tensor), batch_size=64, shuffle=True)
        for _ in range(20): # EPOCH
            for batch in dataloader:
                x = batch[0]
                output = autoencoder(x)
                loss = criterion(output, x)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
        autoencoder.eval()
        with torch.no_grad():
            encoded_data = autoencoder.encoder(X_tensor).cpu().numpy()
    else:
        encoded_data = X_scaled.astype(np.float32)

    aes_key = get_random_bytes(16)
    rsa_key = RSA.generate(2048)
    results = []

    # Ä°lk 50 veri noktasÄ± iÃ§in test
    for i in range(50):
        vector = encoded_data[i].astype(np.float32).tobytes()
        vector_huff = huffman_encode(vector)

        for method in ["AES", "RSA"]:
            for compress in [False, True]:
                data = vector_huff if compress else vector
                #process = psutil.Process(os.getpid())
                #mem_before = process.memory_info().rss #/ 1024
                #start = time.perf_counter()
                
                
                tracemalloc.start()
                start = time.perf_counter()
                if method == "AES":
                    _ = encrypt_aes(data, aes_key)
                else:
                    _ = encrypt_rsa(data, rsa_key.publickey())
                    
                    
                end = time.perf_counter()
                current, peak = tracemalloc.get_traced_memory()
                tracemalloc.stop()
                    
                #end = time.perf_counter()
                #mem_after = process.memory_info().rss #/ 1024
                results.append({
                    "Autoencoder": "Var" if use_autoencoder else "Yok",
                    "Åifreleme": method,
                    "SÄ±kÄ±ÅŸtÄ±rma": "Huffman" if compress else "Yok",
                    "Zaman (s)": end - start,
                    "Bellek (Bytes)": peak - current,
                    "Veri boyutu": len(data),
                    "Anahtar UzayÄ±": 2 ** (aes_key.__len__() * 8) if method == "AES" else 2 ** rsa_key.size_in_bits()
                })

    
    return pd.DataFrame(results), pd.DataFrame(X_scaled)

# =========================
# STREAMLIT
# =========================
st.title("ğŸ” Åifreleme KarÅŸÄ±laÅŸtÄ±rma UygulamasÄ±")

# Autoencoder kullanÄ±larak ve kullanÄ±lmadan ayrÄ± ayrÄ± test yapÄ±lÄ±r
df1, original1 = run_test(use_autoencoder=True)
df2, original2 = run_test(use_autoencoder=False)

df = pd.concat([df1, df2])
original_data = pd.concat([original1, original2])

st.subheader("ğŸ“Œ Ã–rnek Girdi Verisi")
st.write("AÅŸaÄŸÄ±da modelin iÅŸlediÄŸi 540 boyutlu verilerden Ã¶rnekler gÃ¶sterilmektedir:")
st.dataframe(original_data.sample(5))

# SonuÃ§larÄ±n gruplandÄ±rÄ±lmÄ±ÅŸ ortalamalarÄ±nÄ± Ã§Ä±kar
st.subheader("ğŸ“Š Ortalama SonuÃ§lar")
mean_df = df.groupby(["Autoencoder", "Åifreleme", "SÄ±kÄ±ÅŸtÄ±rma"]).agg({
    "Zaman (s)": "mean",
    "Bellek (Bytes)": "mean",
    "Veri boyutu": "mean",
    "Anahtar UzayÄ±": "first"   # DeÄŸiÅŸmediÄŸi iÃ§in tek bir deÄŸer alÄ±nabilir
}).reset_index()

# Anahtar UzayÄ± Ã§ok bÃ¼yÃ¼k int olduÄŸundan string'e dÃ¶nÃ¼ÅŸtÃ¼rÃœR
mean_df["Anahtar UzayÄ±"] = mean_df["Anahtar UzayÄ±"].astype(str)
st.dataframe(mean_df)

# Zaman ve bellek kullanÄ±mÄ± grafikleri
st.subheader("ğŸ–¼ï¸ Zaman ve Bellek KullanÄ±mÄ±")
fig, ax = plt.subplots(1, 2, figsize=(14, 5))
sns.barplot(data=mean_df, x="Åifreleme", y="Zaman (s)", hue="Autoencoder", ax=ax[0])
ax[0].set_title("Zaman KarÅŸÄ±laÅŸtÄ±rmasÄ±")
sns.barplot(data=mean_df, x="Åifreleme", y="Bellek (Bytes)", hue="Autoencoder", ax=ax[1])
ax[1].set_title("Bellek KullanÄ±mÄ±")
st.pyplot(fig)